{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5543f020-e8ab-4b3d-997b-f11333528aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utils/ folder added to Python import path\n",
      "project_root:     C:\\Users\\akaas\\crime-projectMain\n",
      "raw_dir:          C:\\Users\\akaas\\crime-projectMain\\data\n",
      "parquet_dir:      C:\\Users\\akaas\\crime-projectMain\\data_parquet\n",
      "processed_dir:    C:\\Users\\akaas\\crime-projectMain\\data_processed\n",
      "models_dir:       C:\\Users\\akaas\\crime-projectMain\\models\n",
      "logs_dir:         C:\\Users\\akaas\\crime-projectMain\\logs\n",
      "utils_dir:        C:\\Users\\akaas\\crime-projectMain\\utils\n"
     ]
    }
   ],
   "source": [
    "    %run ../notebooks/00_setup_paths.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c14176-6350-49d7-a740-1ca4ceb5d4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Initialized: CrimeProject_Phase2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Akaash.lan:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CrimeProject_Phase2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2c24e3546a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spark_init import init_spark\n",
    "\n",
    "spark = init_spark(\"CrimeProject_Phase2\", driver_memory=\"12g\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5225619-4ae2-4785-a751-159756ee7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, collect_set, collect_list, count, sum as spark_sum\n",
    ")\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32cae05f-38cd-40b0-b116-d31eca1efa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading administrative_segment (11 files)\n",
      "Loading arrestee_segment (11 files)\n",
      "Loading batch_header (1 files)\n",
      "Loading group_b_arrest_report_segment (11 files)\n",
      "Loading offender_segment (11 files)\n",
      "Loading offense_segment (11 files)\n",
      "Loading property_segment (11 files)\n",
      "Loading victim_segment (11 files)\n",
      "Loading window_arrestee_segment (9 files)\n",
      "Loading window_exceptional_clearance_segment (9 files)\n",
      "Loading window_recovered_property_segment (9 files)\n",
      "All segments loaded into segment_dfs dictionary.\n"
     ]
    }
   ],
   "source": [
    "segment_dfs = {}\n",
    "\n",
    "for segment_folder in parquet_dir.iterdir():\n",
    "    if segment_folder.is_dir():\n",
    "        files = list(segment_folder.glob(\"*.parquet\"))\n",
    "        if not files:\n",
    "            continue\n",
    "\n",
    "        print(f\"Loading {segment_folder.name} ({len(files)} files)\")\n",
    "        df = spark.read.parquet(*[str(f) for f in files])\n",
    "        segment_dfs[segment_folder.name] = df\n",
    "\n",
    "print(\"All segments loaded into segment_dfs dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d6bfcb-d380-4d56-b4b9-7b1a0012c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "#Loads all parquet files in a folder.Ensures schema consistency\n",
    "def safe_load_and_cast(segment_folder):\n",
    "    files = list(segment_folder.glob(\"*.parquet\"))\n",
    "    dfs = []\n",
    "    \n",
    "    # Step 1: Detect all columns that appear in any file\n",
    "    all_cols = set()\n",
    "    for f in files:\n",
    "        temp = spark.read.parquet(str(f))\n",
    "        all_cols |= set(temp.columns)\n",
    "    \n",
    "    # Step 2: Load each file individually + cast all cols as string\n",
    "    for f in files:\n",
    "        d = spark.read.parquet(str(f))\n",
    "        \n",
    "        # add missing columns\n",
    "        for c in all_cols:\n",
    "            if c not in d.columns:\n",
    "                d = d.withColumn(c, lit(None))\n",
    "        \n",
    "        # cast all columns to STRING\n",
    "        for c in all_cols:\n",
    "            d = d.withColumn(c, col(c).cast(\"string\"))\n",
    "        \n",
    "        dfs.append(d.select(sorted(all_cols)))\n",
    "    \n",
    "    # Step 3: Union all normalized dfs\n",
    "    final_df = dfs[0]\n",
    "    for d in dfs[1:]:\n",
    "        final_df = final_df.unionByName(d)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19304a26-4945-4609-8a2f-34c888e404e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing administrative_segment safely…\n",
      "Saved normalized: administrative_segment \n",
      "\n",
      "Processing arrestee_segment safely…\n",
      "Saved normalized: arrestee_segment \n",
      "\n",
      "Processing batch_header safely…\n",
      "Saved normalized: batch_header \n",
      "\n",
      "Processing group_b_arrest_report_segment safely…\n",
      "Saved normalized: group_b_arrest_report_segment \n",
      "\n",
      "Processing offender_segment safely…\n",
      "Saved normalized: offender_segment \n",
      "\n",
      "Processing offense_segment safely…\n",
      "Saved normalized: offense_segment \n",
      "\n",
      "Processing property_segment safely…\n",
      "Saved normalized: property_segment \n",
      "\n",
      "Processing victim_segment safely…\n",
      "Saved normalized: victim_segment \n",
      "\n",
      "Processing window_arrestee_segment safely…\n",
      "Saved normalized: window_arrestee_segment \n",
      "\n",
      "Processing window_exceptional_clearance_segment safely…\n",
      "Saved normalized: window_exceptional_clearance_segment \n",
      "\n",
      "Processing window_recovered_property_segment safely…\n",
      "Saved normalized: window_recovered_property_segment \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "normalized_dir = processed_dir / \"normalized\"\n",
    "normalized_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for segment, df in segment_dfs.items():\n",
    "    print(f\"Processing {segment} safely…\")\n",
    "\n",
    "    out_dir = normalized_dir / segment\n",
    "    if out_dir.exists():\n",
    "        shutil.rmtree(out_dir)\n",
    "\n",
    "    safe_df = safe_load_and_cast(parquet_dir / segment)\n",
    "\n",
    "    safe_df.write.mode(\"overwrite\").parquet(str(out_dir))\n",
    "    print(f\"Saved normalized: {segment} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c2a0e49-e163-4a9f-80d3-c395b197ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin = spark.read.parquet(str(normalized_dir / \"administrative_segment\"))\n",
    "offense = spark.read.parquet(str(normalized_dir / \"offense_segment\"))\n",
    "victim = spark.read.parquet(str(normalized_dir / \"victim_segment\"))\n",
    "offender = spark.read.parquet(str(normalized_dir / \"offender_segment\"))\n",
    "property_df = spark.read.parquet(str(normalized_dir / \"property_segment\"))\n",
    "arrestee = spark.read.parquet(str(normalized_dir / \"arrestee_segment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35e34844-a4d1-43cc-b548-680e082672e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "offense_agg = (\n",
    "    offense.groupBy(\"unique_incident_id\")\n",
    "    .agg(\n",
    "        collect_set(\"ucr_offense_code\").alias(\"offense_codes\"),\n",
    "        count(\"*\").alias(\"num_offenses\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dce0795-4928-4c6a-adea-234472801a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "victim_agg = (\n",
    "    victim.groupBy(\"unique_incident_id\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_victims\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "314f9ee3-c6c0-4f83-9536-f946cf8be0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "offender_agg = (\n",
    "    offender.groupBy(\"unique_incident_id\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_offenders\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aae13229-4de8-44d5-9d19-c5fe6d9cf717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum, expr\n",
    "\n",
    "property_agg = (\n",
    "    property_df\n",
    "    .withColumn(\n",
    "        \"value_of_property_num\",\n",
    "        expr(\"coalesce(try_cast(value_of_property AS double), 0)\")\n",
    "    )\n",
    "    .groupBy(\"unique_incident_id\")\n",
    "    .agg(\n",
    "        spark_sum(\"value_of_property_num\").alias(\"total_property_value\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "025921cf-33a2-4b72-a4f0-c09120a45670",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrestee_agg = (\n",
    "    arrestee.groupBy(\"unique_incident_id\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_arrestees\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33bf24df-13d5-4133-8e58-4d68a1a47d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master table created.\n",
      "root\n",
      " |-- unique_incident_id: string (nullable = true)\n",
      " |-- city_submissions: string (nullable = true)\n",
      " |-- cleared_exceptionally: string (nullable = true)\n",
      " |-- exceptional_clearance_date: string (nullable = true)\n",
      " |-- incident_date: string (nullable = true)\n",
      " |-- incident_date_hour: string (nullable = true)\n",
      " |-- incident_number: string (nullable = true)\n",
      " |-- ori: string (nullable = true)\n",
      " |-- report_date_indicator: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- state_abb: string (nullable = true)\n",
      " |-- total_arrestee_segments: string (nullable = true)\n",
      " |-- total_offender_segments: string (nullable = true)\n",
      " |-- total_offense_segments: string (nullable = true)\n",
      " |-- total_victim_segments: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- offense_codes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- num_offenses: long (nullable = true)\n",
      " |-- num_victims: long (nullable = true)\n",
      " |-- num_offenders: long (nullable = true)\n",
      " |-- total_property_value: double (nullable = true)\n",
      " |-- num_arrestees: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "incident_master = (\n",
    "    admin\n",
    "    .join(offense_agg, \"unique_incident_id\", \"left\")\n",
    "    .join(victim_agg, \"unique_incident_id\", \"left\")\n",
    "    .join(offender_agg, \"unique_incident_id\", \"left\")\n",
    "    .join(property_agg, \"unique_incident_id\", \"left\")\n",
    "    .join(arrestee_agg, \"unique_incident_id\", \"left\")\n",
    ")\n",
    "\n",
    "print(\"Master table created.\")\n",
    "incident_master.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8710cbb9-721b-4aaa-9bca-eb47714b1be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master incident table saved to: C:\\Users\\akaas\\crime-projectMain\\data_processed\\incidents_master\n"
     ]
    }
   ],
   "source": [
    "master_dir = processed_dir / \"incidents_master\"\n",
    "master_dir.mkdir(exist_ok=True)\n",
    "incident_master.write.mode(\"overwrite\").parquet(str(master_dir))\n",
    "print(f\"Master incident table saved to: {master_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f703c879-15d6-4b3c-8578-756b9d46eab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Crime Project (PySpark + GPU)",
   "language": "python",
   "name": "crime-project-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
